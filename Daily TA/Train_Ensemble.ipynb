{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/ProgramData/Anaconda3/envs/tf-gpu/Lib/site-packages')\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Dropout, Activation, LeakyReLU, BatchNormalization\n",
    "from keras import losses\n",
    "\n",
    "#from tensorflow.python.keras.callbacks import TensorBoard\n",
    "#from time import time\n",
    "#tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "\n",
    "#Get constants\n",
    "%run ./CONSTANTS.ipynb\n",
    "PERIODS = GET_PERIODS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_given_symbol(symb):\n",
    "    \n",
    "    #Get the dataframe from our function Request.ipynb\n",
    "    %run ./Request.ipynb\n",
    "    raw_data_df = get_symbol_data_df(symb)\n",
    "    \n",
    "    #Pull out the goals and percent change\n",
    "    goals_np_arr = raw_data_df['goal'].values\n",
    "    pc_np_arr = raw_data_df['percent_change'].values\n",
    "    \n",
    "    #Pull out the input data (don't include the percent change or the goals) as a numpy array\n",
    "    inputs_np_arr = raw_data_df.copy().drop(columns=['goal', 'percent_change']).values\n",
    "    \n",
    "    #Add previous periods onto each row and normalize\n",
    "    inputs_np_arr = period_expand_np_arr(inputs_np_arr)\n",
    "    inputs_np_arr = normalize_np_arr(inputs_np_arr)\n",
    "    \n",
    "    #We have to make sure the goal column matches the rows still!\n",
    "    goals_np_arr = goals_np_arr[PERIODS - 1:]\n",
    "    pc_np_arr = pc_np_arr[PERIODS - 1:]\n",
    "    \n",
    "    #We have to put the goals back for the test/train split to work\n",
    "    full_data_np_arr = np.append(inputs_np_arr, np.reshape(goals_np_arr, (-1, 1)), 1)\n",
    "    full_data_np_arr = np.append(full_data_np_arr, np.reshape(pc_np_arr, (-1, 1)), 1)\n",
    "    \n",
    "    #Train the models\n",
    "    dnn_model = train_dnn_classifier(full_data_np_arr)\n",
    "    rf_model = train_rf_classifier(full_data_np_arr)\n",
    "    \n",
    "    return dnn_model, rf_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add previous periods onto each row\n",
    "\n",
    "def period_expand_np_arr(raw_np_arr):\n",
    "    \n",
    "    #Create an new array that has the <PERIODS> previous periods included on the row\n",
    "    full_np_arr = np.empty((np.size(raw_np_arr, 0) - PERIODS + 1, (np.size(raw_np_arr, 1) * PERIODS)))\n",
    "    \n",
    "    #We want to exclude <PREV_PERIODS> documents from the start of our new doc because they don't have enough previous periods to \n",
    "    #make a full multi-period row\n",
    "    for rowIndex in range(len(raw_np_arr[(PERIODS-1):])):\n",
    "        \n",
    "        #Despite rowIndex looking like it starts at PERIODS, like what makes sense\n",
    "        #It starts at 0 and just removes PERIODS elements from the end\n",
    "        #So we get to add PERIODS to the index so it makes sense\n",
    "        realRowIndex = rowIndex + PERIODS - 1\n",
    "        \n",
    "        #Start the new row with our existing row\n",
    "        newRow = raw_np_arr[realRowIndex]\n",
    "    \n",
    "        #Iterate over the last few rows and append them to our new row\n",
    "        for i in range(1, PERIODS):\n",
    "            newRow = np.append(newRow, raw_np_arr[realRowIndex - i])\n",
    "    \n",
    "        #assign the row to our new index\n",
    "        #We'll use the unadjusted rowIndex to base it around 0\n",
    "        full_np_arr[rowIndex] = newRow\n",
    "    \n",
    "    return full_np_arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize a numpy array\n",
    "\n",
    "def normalize_np_arr(np_arr):\n",
    "\n",
    "    #Use Keras' normalization\n",
    "    new_np_arr = keras.utils.normalize(np_arr, axis=0, order=2)\n",
    "    \n",
    "    return new_np_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dnn_classifier(data_np_arr):\n",
    "    (train, test) = train_test_split(data_np_arr, test_size=0.25, random_state=13)\n",
    "\n",
    "    x_train = train[:,:-2]\n",
    "    x_test = test[:,:-2]\n",
    "\n",
    "    y_train = train[:,-2]\n",
    "    y_test = test[:,-2]\n",
    "    \n",
    "    #NN - Binary classifier\n",
    "    model = Sequential()\n",
    "    model.add(Dense(x_train.shape[1], input_dim=x_train.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(125, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(75, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              epochs=100,\n",
    "              batch_size=200)\n",
    "    \n",
    "    score = model.evaluate(x_test, y_test, batch_size=128)\n",
    "    print('The NN model has accuracy stats:', score)\n",
    "    \n",
    "    #model.save('dnn_classifier.h5')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf_classifier(data_np_arr):\n",
    "    \n",
    "    (train, test) = train_test_split(data_np_arr, test_size=0.25, random_state=13)\n",
    "\n",
    "    x_train = train[:,:-2]\n",
    "    x_test = test[:,:-2]\n",
    "\n",
    "    y_train = train[:,-2]\n",
    "    y_test = test[:,-2]\n",
    "    \n",
    "    parameters = {'bootstrap': True,\n",
    "              'min_samples_leaf': 3,\n",
    "              'n_estimators': 500, \n",
    "              'min_samples_split': 5,\n",
    "              'max_features': 'sqrt',\n",
    "              'max_depth': 500,\n",
    "              'max_leaf_nodes': None}\n",
    "\n",
    "    RF_model = RandomForestClassifier(**parameters)\n",
    "    \n",
    "    RF_model.fit(x_train, y_train)\n",
    "    \n",
    "    #Test the accuracy\n",
    "    RF_predictions = RF_model.predict(x_test)\n",
    "    score = accuracy_score(y_test, RF_predictions)\n",
    "    print('The RF model has accuracy stats:', score)\n",
    "    \n",
    "    #SAVE MODEL\n",
    "    filename = 'rf_model.sav'\n",
    "    pickle.dump(RF_model, open(filename, 'wb'))\n",
    "    \n",
    "    return RF_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3626/3626 [==============================] - 4s 1ms/step - loss: 0.6985 - acc: 0.5121\n",
      "Epoch 2/100\n",
      "3626/3626 [==============================] - 0s 68us/step - loss: 0.6952 - acc: 0.5047\n",
      "Epoch 3/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6947 - acc: 0.5088\n",
      "Epoch 4/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6962 - acc: 0.5052\n",
      "Epoch 5/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6938 - acc: 0.5085\n",
      "Epoch 6/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6953 - acc: 0.5072\n",
      "Epoch 7/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6954 - acc: 0.5069\n",
      "Epoch 8/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6939 - acc: 0.5025\n",
      "Epoch 9/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.6928 - acc: 0.5229\n",
      "Epoch 10/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.6945 - acc: 0.4972\n",
      "Epoch 11/100\n",
      "3626/3626 [==============================] - 0s 69us/step - loss: 0.6946 - acc: 0.5077\n",
      "Epoch 12/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6942 - acc: 0.5108\n",
      "Epoch 13/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.6936 - acc: 0.5099\n",
      "Epoch 14/100\n",
      "3626/3626 [==============================] - 0s 68us/step - loss: 0.6946 - acc: 0.5127\n",
      "Epoch 15/100\n",
      "3626/3626 [==============================] - 0s 64us/step - loss: 0.6925 - acc: 0.5099\n",
      "Epoch 16/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6930 - acc: 0.5072\n",
      "Epoch 17/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.6942 - acc: 0.4981\n",
      "Epoch 18/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6933 - acc: 0.5091\n",
      "Epoch 19/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6938 - acc: 0.5033\n",
      "Epoch 20/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6935 - acc: 0.5102\n",
      "Epoch 21/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6930 - acc: 0.5171\n",
      "Epoch 22/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6930 - acc: 0.5204\n",
      "Epoch 23/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6940 - acc: 0.5080\n",
      "Epoch 24/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6937 - acc: 0.5127\n",
      "Epoch 25/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6928 - acc: 0.5113\n",
      "Epoch 26/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6927 - acc: 0.5204\n",
      "Epoch 27/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.6933 - acc: 0.5094\n",
      "Epoch 28/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6935 - acc: 0.5047\n",
      "Epoch 29/100\n",
      "3626/3626 [==============================] - 0s 64us/step - loss: 0.6927 - acc: 0.5248\n",
      "Epoch 30/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6933 - acc: 0.5177\n",
      "Epoch 31/100\n",
      "3626/3626 [==============================] - 0s 68us/step - loss: 0.6930 - acc: 0.5149\n",
      "Epoch 32/100\n",
      "3626/3626 [==============================] - 0s 72us/step - loss: 0.6934 - acc: 0.5099\n",
      "Epoch 33/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6927 - acc: 0.5185\n",
      "Epoch 34/100\n",
      "3626/3626 [==============================] - 0s 68us/step - loss: 0.6924 - acc: 0.5215\n",
      "Epoch 35/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6945 - acc: 0.5154\n",
      "Epoch 36/100\n",
      "3626/3626 [==============================] - 0s 68us/step - loss: 0.6927 - acc: 0.5152\n",
      "Epoch 37/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6925 - acc: 0.5138\n",
      "Epoch 38/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6924 - acc: 0.5190\n",
      "Epoch 39/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6918 - acc: 0.5223\n",
      "Epoch 40/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.6908 - acc: 0.5347\n",
      "Epoch 41/100\n",
      "3626/3626 [==============================] - 0s 69us/step - loss: 0.6915 - acc: 0.5279\n",
      "Epoch 42/100\n",
      "3626/3626 [==============================] - 0s 69us/step - loss: 0.6896 - acc: 0.5334\n",
      "Epoch 43/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.6896 - acc: 0.5350\n",
      "Epoch 44/100\n",
      "3626/3626 [==============================] - 0s 70us/step - loss: 0.6901 - acc: 0.5303\n",
      "Epoch 45/100\n",
      "3626/3626 [==============================] - 0s 69us/step - loss: 0.6881 - acc: 0.5447\n",
      "Epoch 46/100\n",
      "3626/3626 [==============================] - 0s 71us/step - loss: 0.6866 - acc: 0.5560\n",
      "Epoch 47/100\n",
      "3626/3626 [==============================] - 0s 68us/step - loss: 0.6850 - acc: 0.5436\n",
      "Epoch 48/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6802 - acc: 0.5725\n",
      "Epoch 49/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6808 - acc: 0.5474\n",
      "Epoch 50/100\n",
      "3626/3626 [==============================] - 0s 71us/step - loss: 0.6805 - acc: 0.5706\n",
      "Epoch 51/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6808 - acc: 0.5712\n",
      "Epoch 52/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6768 - acc: 0.5703\n",
      "Epoch 53/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6718 - acc: 0.5797\n",
      "Epoch 54/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.6747 - acc: 0.5720\n",
      "Epoch 55/100\n",
      "3626/3626 [==============================] - 0s 68us/step - loss: 0.6705 - acc: 0.5869\n",
      "Epoch 56/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6654 - acc: 0.5943\n",
      "Epoch 57/100\n",
      "3626/3626 [==============================] - 0s 68us/step - loss: 0.6680 - acc: 0.5894\n",
      "Epoch 58/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6560 - acc: 0.6001\n",
      "Epoch 59/100\n",
      "3626/3626 [==============================] - 0s 69us/step - loss: 0.6628 - acc: 0.5918\n",
      "Epoch 60/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6580 - acc: 0.5946\n",
      "Epoch 61/100\n",
      "3626/3626 [==============================] - 0s 68us/step - loss: 0.6519 - acc: 0.6059\n",
      "Epoch 62/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6494 - acc: 0.6100\n",
      "Epoch 63/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6461 - acc: 0.6117\n",
      "Epoch 64/100\n",
      "3626/3626 [==============================] - 0s 64us/step - loss: 0.6510 - acc: 0.6133\n",
      "Epoch 65/100\n",
      "3626/3626 [==============================] - 0s 68us/step - loss: 0.6416 - acc: 0.6103\n",
      "Epoch 66/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.6379 - acc: 0.6280\n",
      "Epoch 67/100\n",
      "3626/3626 [==============================] - 0s 68us/step - loss: 0.6360 - acc: 0.6202\n",
      "Epoch 68/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6347 - acc: 0.6191\n",
      "Epoch 69/100\n",
      "3626/3626 [==============================] - 0s 69us/step - loss: 0.6449 - acc: 0.6219\n",
      "Epoch 70/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6199 - acc: 0.6470\n",
      "Epoch 71/100\n",
      "3626/3626 [==============================] - 0s 68us/step - loss: 0.6291 - acc: 0.6258\n",
      "Epoch 72/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.6225 - acc: 0.6255\n",
      "Epoch 73/100\n",
      "3626/3626 [==============================] - 0s 68us/step - loss: 0.6156 - acc: 0.6365\n",
      "Epoch 74/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6222 - acc: 0.6338\n",
      "Epoch 75/100\n",
      "3626/3626 [==============================] - 0s 68us/step - loss: 0.6135 - acc: 0.6517\n",
      "Epoch 76/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.6170 - acc: 0.6395\n",
      "Epoch 77/100\n",
      "3626/3626 [==============================] - 0s 69us/step - loss: 0.6158 - acc: 0.6349\n",
      "Epoch 78/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6188 - acc: 0.6481\n",
      "Epoch 79/100\n",
      "3626/3626 [==============================] - 0s 68us/step - loss: 0.6067 - acc: 0.6522\n",
      "Epoch 80/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.6112 - acc: 0.6409\n",
      "Epoch 81/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.6118 - acc: 0.6451\n",
      "Epoch 82/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.5984 - acc: 0.6498\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3626/3626 [==============================] - 0s 68us/step - loss: 0.5948 - acc: 0.6602\n",
      "Epoch 84/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.5859 - acc: 0.6627\n",
      "Epoch 85/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.6070 - acc: 0.6514\n",
      "Epoch 86/100\n",
      "3626/3626 [==============================] - 0s 66us/step - loss: 0.5852 - acc: 0.6671\n",
      "Epoch 87/100\n",
      "3626/3626 [==============================] - 0s 70us/step - loss: 0.6063 - acc: 0.6509\n",
      "Epoch 88/100\n",
      "3626/3626 [==============================] - 0s 64us/step - loss: 0.5894 - acc: 0.6561\n",
      "Epoch 89/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.5901 - acc: 0.6649\n",
      "Epoch 90/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.5818 - acc: 0.6804\n",
      "Epoch 91/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.5776 - acc: 0.6726\n",
      "Epoch 92/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.5592 - acc: 0.6768\n",
      "Epoch 93/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.5914 - acc: 0.6619\n",
      "Epoch 94/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.5750 - acc: 0.6779\n",
      "Epoch 95/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.5730 - acc: 0.6828\n",
      "Epoch 96/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.5673 - acc: 0.6936\n",
      "Epoch 97/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.5626 - acc: 0.6875\n",
      "Epoch 98/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.5614 - acc: 0.6928\n",
      "Epoch 99/100\n",
      "3626/3626 [==============================] - 0s 65us/step - loss: 0.5630 - acc: 0.6953\n",
      "Epoch 100/100\n",
      "3626/3626 [==============================] - 0s 67us/step - loss: 0.5465 - acc: 0.7016\n",
      "1209/1209 [==============================] - 0s 92us/step\n",
      "The NN model has accuracy stats: [0.963755630480543, 0.516129031715756]\n",
      "The RF model has accuracy stats: 0.5045492142266336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.sequential.Sequential at 0x14a6c0617f0>,\n",
       " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=500, max_features='sqrt', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=3, min_samples_split=5,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_given_symbol('A')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
